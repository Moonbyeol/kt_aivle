```python
import keras

from keras.utils import clear_session
from keras.models import Sequential, Model
from keras.layers import Input, Dense, Flatten, BatchNormalization, Dropout
from keras.layers import Conv2D, MaxPool2D, RandomFlip
from keras.optimizers import Adam
from keras.losses import categorical_crossentropy
```
# Augmentation Layer
```python
keras.layers.RandomRotation(factor=(-0.3,0.3)),
keras.layers.RandomTranslation(height_factor=(-0.3,0.3), width_factor=(-0.3,0.3)),
keras.layers.RandomZoom(height_factor=(-0.2,0.2), width_factor=(-0.2,0.2)),
keras.layers.RandomFlip(mode='horizontal_and_vertical')
```

# Scaling(min-max scaling)
```python
min_n, max_n = train_x.min(), train_x.max()

train_x = (train_x - min_n) / (max_n - min_n)
val_x = (val_x - min_n) / (max_n - min_n)
test_x = (test_x - min_n) / (max_n - min_n)
```


# Sequential API
```python
## Sequential API
# 1 세션 클리어
clear_session()

# 2 모델 선언
model = Sequential()

# 3 레이어 조립
model.add( Input(shape=(32,32,3)) )

####################################
# Augmentation Layer
model.add( RandomFlip(mode='vertical') )
####################################

model.add( Conv2D(filters=64,        # 서로 다른 64개의 filter를 사용하여 새로운 feature map을 만들겠다는 의미
                  kernel_size=(3,3), # Conv2D filter의 가로세로 사이즈
                  strides=(1,1),     # Conv2D filter의 이동 보폭
                  padding='same',    # 1.feature map 크기 유지 및 외곽 정보 더 반영
                  activation='relu', # 주의!
                  ) )

model.add( Conv2D(filters=64,        # 서로 다른 64개의 filter를 사용하여 새로운 feature map을 만들겠다는 의미
                  kernel_size=(3,3), # Conv2D filter의 가로세로 사이즈
                  strides=(1,1),     # Conv2D filter의 이동 보폭
                  padding='same',    # 1.feature map 크기 유지 및 외곽 정보 더 반영
                  activation='relu', # 주의!
                  ) )

model.add( MaxPool2D(pool_size=(2,2),# pooling filter의 가로세로 사이즈
                     strides=(2,2),  # pooling filter의 이동 보폭
                     ) )

model.add( BatchNormalization() )

model.add( Dropout(0.4) )

model.add( Conv2D(filters=128,        # 서로 다른 128개의 filter를 사용하여 새로운 feature map을 만들겠다는 의미
                  kernel_size=(3,3), # Conv2D filter의 가로세로 사이즈
                  strides=(1,1),     # Conv2D filter의 이동 보폭
                  padding='same',    # 1.feature map 크기 유지 및 외곽 정보 더 반영
                  activation='relu', # 주의!
                  ) )

model.add( Conv2D(filters=128,        # 서로 다른 128개의 filter를 사용하여 새로운 feature map을 만들겠다는 의미
                  kernel_size=(3,3), # Conv2D filter의 가로세로 사이즈
                  strides=(1,1),     # Conv2D filter의 이동 보폭
                  padding='same',    # 1.feature map 크기 유지 및 외곽 정보 더 반영
                  activation='relu', # 주의!
                  ) )

model.add( MaxPool2D(pool_size=(2,2),# pooling filter의 가로세로 사이즈
                     strides=(2,2),  # pooling filter의 이동 보폭
                     ) )

model.add( BatchNormalization() )

model.add( Dropout(0.4) )

model.add( Conv2D(filters=256,        # 서로 다른 256개의 filter를 사용하여 새로운 feature map을 만들겠다는 의미
                  kernel_size=(3,3), # Conv2D filter의 가로세로 사이즈
                  strides=(1,1),     # Conv2D filter의 이동 보폭
                  padding='same',    # 1.feature map 크기 유지 및 외곽 정보 더 반영
                  activation='relu', # 주의!
                  ) )

model.add( Conv2D(filters=256,        # 서로 다른 256개의 filter를 사용하여 새로운 feature map을 만들겠다는 의미
                  kernel_size=(3,3), # Conv2D filter의 가로세로 사이즈
                  strides=(1,1),     # Conv2D filter의 이동 보폭
                  padding='same',    # 1.feature map 크기 유지 및 외곽 정보 더 반영
                  activation='relu', # 주의!
                  ) )

model.add( MaxPool2D(pool_size=(2,2),# pooling filter의 가로세로 사이즈
                     strides=(2,2),  # pooling filter의 이동 보폭
                     ) )

model.add( BatchNormalization() )

model.add( Dropout(0.4) )

model.add( Flatten() )

model.add( Dense(100, activation='softmax') )

# 4 컴파일
model.compile(optimizer='adam', loss=keras.losses.sparse_categorical_crossentropy,
              metrics=['accuracy'])

model.summary()
```

