```python
import keras

from keras.utils import clear_session
from keras.models import Sequential, Model
from keras.layers import Input, Dense, Flatten, BatchNormalization, Dropout
from keras.layers import Conv2D, MaxPool2D, RandomFlip
from keras.optimizers import Adam
from keras.losses import categorical_crossentropy
```
# Augmentation Layer
```python
keras.layers.RandomRotation(factor=(-0.3,0.3)),
keras.layers.RandomTranslation(height_factor=(-0.3,0.3), width_factor=(-0.3,0.3)),
keras.layers.RandomZoom(height_factor=(-0.2,0.2), width_factor=(-0.2,0.2)),
keras.layers.RandomFlip(mode='horizontal_and_vertical')
```

# Scaling(min-max scaling)
```python
min_n, max_n = train_x.min(), train_x.max()

train_x = (train_x - min_n) / (max_n - min_n)
val_x = (val_x - min_n) / (max_n - min_n)
test_x = (test_x - min_n) / (max_n - min_n)
```

# One-Hot Encoding
```

```

# Sequential API
```python
## Sequential API
# 1 세션 클리어
clear_session()

# 2 모델 선언
model = Sequential()

# 3 레이어 조립
model.add( Input(shape=(32,32,3)) )

####################################
# Augmentation Layer
model.add( RandomFlip(mode='vertical') )
####################################

model.add( Conv2D(filters=64,        # 서로 다른 64개의 filter를 사용하여 새로운 feature map을 만들겠다는 의미
                  kernel_size=(3,3), # Conv2D filter의 가로세로 사이즈
                  strides=(1,1),     # Conv2D filter의 이동 보폭
                  padding='same',    # 1.feature map 크기 유지 및 외곽 정보 더 반영
                  activation='relu', # 주의!
                  ) )

model.add( Conv2D(filters=64,        # 서로 다른 64개의 filter를 사용하여 새로운 feature map을 만들겠다는 의미
                  kernel_size=(3,3), # Conv2D filter의 가로세로 사이즈
                  strides=(1,1),     # Conv2D filter의 이동 보폭
                  padding='same',    # 1.feature map 크기 유지 및 외곽 정보 더 반영
                  activation='relu', # 주의!
                  ) )

model.add( MaxPool2D(pool_size=(2,2),# pooling filter의 가로세로 사이즈
                     strides=(2,2),  # pooling filter의 이동 보폭
                     ) )

model.add( BatchNormalization() )

model.add( Dropout(0.4) )

model.add( Conv2D(filters=128,        # 서로 다른 128개의 filter를 사용하여 새로운 feature map을 만들겠다는 의미
                  kernel_size=(3,3), # Conv2D filter의 가로세로 사이즈
                  strides=(1,1),     # Conv2D filter의 이동 보폭
                  padding='same',    # 1.feature map 크기 유지 및 외곽 정보 더 반영
                  activation='relu', # 주의!
                  ) )

model.add( Conv2D(filters=128,        # 서로 다른 128개의 filter를 사용하여 새로운 feature map을 만들겠다는 의미
                  kernel_size=(3,3), # Conv2D filter의 가로세로 사이즈
                  strides=(1,1),     # Conv2D filter의 이동 보폭
                  padding='same',    # 1.feature map 크기 유지 및 외곽 정보 더 반영
                  activation='relu', # 주의!
                  ) )

model.add( MaxPool2D(pool_size=(2,2),# pooling filter의 가로세로 사이즈
                     strides=(2,2),  # pooling filter의 이동 보폭
                     ) )

model.add( BatchNormalization() )

model.add( Dropout(0.4) )

model.add( Conv2D(filters=256,        # 서로 다른 256개의 filter를 사용하여 새로운 feature map을 만들겠다는 의미
                  kernel_size=(3,3), # Conv2D filter의 가로세로 사이즈
                  strides=(1,1),     # Conv2D filter의 이동 보폭
                  padding='same',    # 1.feature map 크기 유지 및 외곽 정보 더 반영
                  activation='relu', # 주의!
                  ) )

model.add( Conv2D(filters=256,        # 서로 다른 256개의 filter를 사용하여 새로운 feature map을 만들겠다는 의미
                  kernel_size=(3,3), # Conv2D filter의 가로세로 사이즈
                  strides=(1,1),     # Conv2D filter의 이동 보폭
                  padding='same',    # 1.feature map 크기 유지 및 외곽 정보 더 반영
                  activation='relu', # 주의!
                  ) )

model.add( MaxPool2D(pool_size=(2,2),# pooling filter의 가로세로 사이즈
                     strides=(2,2),  # pooling filter의 이동 보폭
                     ) )

model.add( BatchNormalization() )

model.add( Dropout(0.4) )

model.add( Flatten() )

model.add( Dense(100, activation='softmax') )

# 4 컴파일
model.compile(optimizer='adam', loss=keras.losses.sparse_categorical_crossentropy,
              metrics=['accuracy'])

model.summary()
```

# Functional API
```python
## Functional API
# 1 세션 클리어
clear_session()

# 2 레이어 연결
il = Input(shape=(32,32,3))

####################################
al = RandomFlip(mode='vertical')(il)
####################################

hl = Conv2D(64, (3,3), (1,1), 'same', activation='relu')(al)
hl = Conv2D(64, (3,3), (1,1), 'same', activation='relu')(hl)
hl = MaxPool2D((2,2), (2,2))(hl)
hl = BatchNormalization()(hl)
hl = Dropout(0.4)(hl)

hl = Conv2D(128, (3,3), (1,1), 'same', activation='relu')(hl)
hl = Conv2D(128, (3,3), (1,1), 'same', activation='relu')(hl)
hl = MaxPool2D((2,2), (2,2))(hl)
hl = BatchNormalization()(hl)
hl = Dropout(0.4)(hl)

hl = Conv2D(256, (3,3), (1,1), 'same', activation='relu')(hl)
hl = Conv2D(256, (3,3), (1,1), 'same', activation='relu')(hl)
hl = MaxPool2D((2,2), (2,2))(hl)
hl = BatchNormalization()(hl)
hl = Dropout(0.4)(hl)

hl = Flatten()(hl)
ol = Dense(100, activation='softmax')(hl)

# 3 모델 시작 끝 지정
model = Model(il, ol)

# 4 컴파일
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()
```

# EarlyStopping
```python
from keras.callbacks import EarlyStopping

es = EarlyStopping(monitor ='val_loss',
                   min_delta=0,
                   patience=5,
                   verbose=1,
                   restore_best_weights=True
                   )
```

# fit
```python
model.fit(train_x, train_y, validation_data=(val_x,val_y),
          epochs=10000, verbose=1,
          callbacks=[es]
          )
```

# 성능 평가
```python
performance_test = model.evaluate(test_x, test_y)

print(f'Test Loss: {performance_test[0]:.6f}')
print(f'Test Accuracy: {performance_test[1]*100:.3f}%')
```

```python
if not isinstance(history, dict):
    history = history.history

plt.plot(history['accuracy'])
plt.plot(history['val_accuracy'])
plt.title('Accuracy : Training vs Validation')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Training', 'Validation'], loc=0)
plt.show()
```
# Visualization
## 실제 데이터 확인
```python
rand_idx = np.random.randint(0, len(y_pred_arg))
test_idx = test_y_arg[rand_idx]
pred_idx = y_pred_arg[rand_idx]
class_prob = np.floor( y_pred[rand_idx]*100 )

print(f'idx = {rand_idx}')
print(f'해당 인덱스의 이미지는 {label_dict[test_idx]}')
print(f'모델의 예측 : {label_dict[pred_idx]}')
print(f'모델의 클래스별 확률 : ')
print('-------------------')

for idx, val in enumerate( list(label_dict.values()) ) :
    print(val, class_prob[idx])

print('=================================================')

if test_y_arg[rand_idx] == y_pred_arg[rand_idx] :
    print('정답')
else :
    print('땡')

plt.imshow(test_x[rand_idx])
plt.show()
```

## 틀린 이미지만 확인
```python
temp = (test_y_arg == y_pred_arg)
false_idx = np.where(temp==False)[0]
false_len = len(false_idx)
false_len
```

```python
rand_idx = false_idx[np.random.randint(0, false_len)]
test_idx = test_y_arg[rand_idx]
pred_idx = y_pred_arg[rand_idx]
class_prob = np.floor( y_pred[rand_idx]*100 )

print(f'idx = {rand_idx}')
print(f'해당 인덱스의 이미지는 {label_dict[test_idx]}')
print(f'모델의 예측 : {label_dict[pred_idx]}')
print(f'모델의 클래스별 확률 : ')
print('-------------------')

for idx, val in enumerate( list(label_dict.values()) ) :
    print(val, class_prob[idx])
print('=================================================')

if test_y_arg[rand_idx] == y_pred_arg[rand_idx] :
    print('정답')
else :
    print('땡')

plt.imshow(test_x[rand_idx] )
plt.show()
```


# model checkpoint
```python
from keras.callbacks import ModelCheckpoint

mcp = ModelCheckpoint(filepath='./model1.keras',      # 모델 저장 경로
                      monitor='val_loss',             # 모델 저장의 관심 대상
                      verbose=1,                      # 어느 시점에서 저장되는지 알려줌
                      save_best_only=True,            # 최고 성능 모델만 저장
                      save_weights_only=False)        # True : 가중치만 저장 .h5 | False : 모델 구조 포함하여 저장 .keras
```

# Transfer Learning
```python
keras.utils.clear_session()

base_model = ResNet50(include_top=False,
                 weights='imagenet',
                 input_shape=(32,32,3),
                 pooling='avg'
                 )
  
output_layer = keras.layers.Dense(10, activation='softmax')(base_model.output)

new_model = keras.models.Model(base_model.input, output_layer)
new_model.summary()
```

