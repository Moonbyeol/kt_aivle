![](https://i.imgur.com/fUcQexe.png)
![](https://i.imgur.com/7UKeuck.png)
![](https://i.imgur.com/Tv83GM7.png)
![](https://i.imgur.com/uMynYBz.png)
![](https://i.imgur.com/lm5sP9m.png)

![](https://i.imgur.com/qFBZfzX.png)



# RNN의 문제
- 기억력이 안좋음
- 갈수록 까먹음

# LSTM
- RNN의 단점 보완



```python
timesteps = 14
x2, y2 = temporalize(x, y, timesteps)
x2.shape, y2.shape

x_train, x_val, y_train, y_val = train_test_split(x2, y2, test_size= 53, shuffle = False)

timesteps = x_train[1]
nfeatures = x_train[2]

clear_session()

model = Sequential([
    LSTM(8, input_shape=(timesteps, nfeatures), return_sequences=True),
    LSTM(8),
    Dense(1)        
])

model.summary()

# 모델 학습
model.compile(optimizer=Adam(0.01), loss='mse')
hist = model.fit(x_train,y_train,epochs = 100, verbose=0, validation_split=.2).history

# 그래프
dl_history_plot(hist)

# 예측
pred = model.predict(x_val)
# 평가
mean_absolute_error(y_val,pred)

# 그래프
plt.figure(figsize = (10,6))
plt.plot(y_val, label = 'actual')
plt.plot(pred, label = 'predicted')
plt.legend()
plt.grid()
plt.show()
```
