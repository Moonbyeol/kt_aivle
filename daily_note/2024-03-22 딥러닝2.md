




# 회귀

## Activation

### relu

## Loss Function

### mse



# 분류

## Activation

### sigmoid
- 이진 분시시

### softmax
- 다중 분류시
- 각 class별 예측한 값을 하나의 확률 값으로 변환해서 반환
- 확률이므로 전체 합 = 1

## Loss Function

### binary_crossentropy
- 이진 분류시

### Separate_binary_crossentropy




# 실습 - 분류

```python
n = x_train.shape[1]

clear_session()

model = Sequential([
    Dense(16, input_shape=(n,),activation='relu'),
    Dense(1, activation='sigmoid')
])

model.summary()

model.compile(optimizer=Adam(0.01), loss ='binary_crossentropy')

model.fit(x_train,y_train, epochs=50, validation_split=.2)

# 그래프
dl_history_plot(history)

# 예측 및 검증
pred = model.predict(x_val)
pred = np.where(pred >= 0.5, 1, 0)
print(classification_report(y_val, pred))
```



# Resampling
- 데이터 불균형 처리
```python
# 불균형 확인
print(data['Attrition'].value_counts(normalize=True))
sns.countplot(x='Attrition', data=data)
plt.grid()
plt.show()


# 불균형 처리
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler()
x_train_ros, y_train_ros = ros.fit_resample(x_train, y_train)

n = x_train_ros.shape[1]

clear_session()

model2 = Sequential([
    Dense(8, input_shape=(n,), activation='relu'),
    Dense(1,activation='sigmoid')
])

model2.summary()

model2.compile(optimizer=Adam(0.01), loss='binary_crossentropy')

hist = model2.fit( x_train_ros, y_train_ros, epochs=50, validation_split=.2).history
```

![](https://i.imgur.com/lfCSpZQ.png)

# 딥러닝 - 다중분류
![](https://i.imgur.com/gQzhuCy.png)

```python
nfeatures = x_train.shape[1] #num of columns
nfeatures

# 메모리 정리
clear_session()

# Sequential
model = Sequential( Dense( 3 , input_shape = (nfeatures,), activation = 'softmax') )

# 모델요약
model.summary()

### (2) compile + 학습

model.compile(optimizer=Adam(learning_rate=0.1), loss= 'sparse_categorical_crossentropy')

history = model.fit(x_train, y_train, epochs = 50, validation_split=0.2).history

* 학습결과 그래프

dl_history_plot(history)

### (3) 예측 및 검증
* 예측 결과는 softmax로 변환된 값 입니다.

pred = model.predict(x_val)
pred[:5]

* 행 별로 제일 큰 값을 찾아서 그에 맞게 숫자(0,1,2)로 변환 합시다.

# 5개 행만 살펴보면
np.argmax(pred[:5], axis = 1)

# 전체에 적용해서 변환합시다.
pred_1 = pred.argmax(axis=1)
pred_1

* 실제값 y_val은 0,1,2 로 된 1차원 값입니다.

y_val

print(confusion_matrix(y_val, pred_1))
print(classification_report(y_val, pred_1))
```

![](https://i.imgur.com/S9Mrieg.png)
